{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "149c7389",
   "metadata": {},
   "source": [
    "## Model for price prediction from sentiment data.\n",
    "> We need it to take a dictionary of sentence : sentiment pairs.\n",
    "> This size of the dictionary is variable/flexible.\n",
    "\n",
    "We could potentially design this module as an Transformer encoder model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4137062f",
   "metadata": {},
   "source": [
    "References:\n",
    "> https://towardsdatascience.com/a-detailed-guide-to-pytorchs-nn-transformer-module-c80afbc9ffb1/\n",
    "\n",
    "> [Set Transformer: A Framework for Attention-based\n",
    " Permutation-Invariant Neural Networks](https://arxiv.org/pdf/1810.00825)\n",
    "\n",
    " > https://github.com/juho-lee/set_transformer\n",
    "\n",
    " > [Deep sets](https://papers.nips.cc/paper_files/paper/2017/hash/f22e4747da1aa27e363d86d40ff442fe-Abstract.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f4beec",
   "metadata": {},
   "source": [
    "Since we want our model to be invarient to permutations in the order in which we feed the news articles and associated sentiments, we can proceed using a set-transformer model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "37ead1dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "class MAB(nn.Module):\n",
    "    def __init__(self, dim_Q, dim_K, dim_V, num_heads, ln=False):\n",
    "        super(MAB, self).__init__()\n",
    "        self.dim_V = dim_V\n",
    "        self.num_heads = num_heads\n",
    "        self.fc_q = nn.Linear(dim_Q, dim_V)\n",
    "        self.fc_k = nn.Linear(dim_K, dim_V)\n",
    "        self.fc_v = nn.Linear(dim_K, dim_V)\n",
    "        if ln:\n",
    "            self.ln0 = nn.LayerNorm(dim_V)\n",
    "            self.ln1 = nn.LayerNorm(dim_V)\n",
    "        self.fc_o = nn.Linear(dim_V, dim_V)\n",
    "\n",
    "    def forward(self, Q, K):\n",
    "        Q = self.fc_q(Q)\n",
    "        K, V = self.fc_k(K), self.fc_v(K)\n",
    "\n",
    "        dim_split = self.dim_V // self.num_heads\n",
    "        Q_ = torch.cat(Q.split(dim_split, 2), 0)\n",
    "        K_ = torch.cat(K.split(dim_split, 2), 0)\n",
    "        V_ = torch.cat(V.split(dim_split, 2), 0)\n",
    "\n",
    "        A = torch.softmax(Q_.bmm(K_.transpose(1,2))/math.sqrt(self.dim_V), 2)\n",
    "        O = torch.cat((Q_ + A.bmm(V_)).split(Q.size(0), 0), 2)\n",
    "        O = O if getattr(self, 'ln0', None) is None else self.ln0(O)\n",
    "        O = O + F.relu(self.fc_o(O))\n",
    "        O = O if getattr(self, 'ln1', None) is None else self.ln1(O)\n",
    "        return O\n",
    "\n",
    "class SAB(nn.Module):\n",
    "    def __init__(self, dim_in, dim_out, num_heads, ln=False):\n",
    "        super(SAB, self).__init__()\n",
    "        self.mab = MAB(dim_in, dim_in, dim_out, num_heads, ln=ln)\n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.mab(X, X)\n",
    "\n",
    "class ISAB(nn.Module):\n",
    "    def __init__(self, dim_in, dim_out, num_heads, num_inds, ln=False):\n",
    "        super(ISAB, self).__init__()\n",
    "        self.I = nn.Parameter(torch.Tensor(1, num_inds, dim_out))\n",
    "        nn.init.xavier_uniform_(self.I)\n",
    "        self.mab0 = MAB(dim_out, dim_in, dim_out, num_heads, ln=ln)\n",
    "        self.mab1 = MAB(dim_in, dim_out, dim_out, num_heads, ln=ln)\n",
    "\n",
    "    def forward(self, X):\n",
    "        H = self.mab0(self.I.repeat(X.size(0), 1, 1), X)\n",
    "        return self.mab1(X, H)\n",
    "\n",
    "class PMA(nn.Module):\n",
    "    def __init__(self, dim, num_heads, num_seeds, ln=False):\n",
    "        super(PMA, self).__init__()\n",
    "        self.S = nn.Parameter(torch.Tensor(1, num_seeds, dim))\n",
    "        nn.init.xavier_uniform_(self.S)\n",
    "        self.mab = MAB(dim, dim, dim, num_heads, ln=ln)\n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.mab(self.S.repeat(X.size(0), 1, 1), X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bde620f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepSet(nn.Module):\n",
    "    def __init__(self, dim_input, num_outputs, dim_output, dim_hidden=128):\n",
    "        super(DeepSet, self).__init__()\n",
    "        self.num_outputs = num_outputs\n",
    "        self.dim_output = dim_output\n",
    "        self.enc = nn.Sequential(\n",
    "                nn.Linear(dim_input, dim_hidden),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(dim_hidden, dim_hidden),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(dim_hidden, dim_hidden),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(dim_hidden, dim_hidden))\n",
    "        self.dec = nn.Sequential(\n",
    "                nn.Linear(dim_hidden, dim_hidden),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(dim_hidden, dim_hidden),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(dim_hidden, dim_hidden),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(dim_hidden, num_outputs*dim_output))\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = self.enc(X).mean(-2)\n",
    "        X = self.dec(X).reshape(-1, self.num_outputs, self.dim_output)\n",
    "        return X\n",
    "\n",
    "class SetTransformer(nn.Module):\n",
    "    def __init__(self, dim_input, num_outputs, dim_output,\n",
    "            num_inds=32, dim_hidden=128, num_heads=4, ln=False):\n",
    "        super(SetTransformer, self).__init__()\n",
    "        self.enc = nn.Sequential(\n",
    "                ISAB(dim_input, dim_hidden, num_heads, num_inds, ln=ln),\n",
    "                ISAB(dim_hidden, dim_hidden, num_heads, num_inds, ln=ln))\n",
    "        self.dec = nn.Sequential(\n",
    "                PMA(dim_hidden, num_heads, num_outputs, ln=ln),\n",
    "                SAB(dim_hidden, dim_hidden, num_heads, ln=ln),\n",
    "                SAB(dim_hidden, dim_hidden, num_heads, ln=ln),\n",
    "                nn.Linear(dim_hidden, dim_output))\n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.dec(self.enc(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "98ee56b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 10, 256) [[[0.70924795 0.32848698 0.82711387 ... 0.49807447 0.90072435 0.8727512 ]\n",
      "  [0.9046867  0.60809374 0.53850275 ... 0.6635218  0.29242167 0.12887849]\n",
      "  [0.5412136  0.41911846 0.6955833  ... 0.5759633  0.15471639 0.8632402 ]\n",
      "  ...\n",
      "  [0.9149834  0.848888   0.8758864  ... 0.23433454 0.3700483  0.7704213 ]\n",
      "  [0.6062975  0.66818774 0.49246314 ... 0.90030676 0.47267076 0.05881489]\n",
      "  [0.10701042 0.9551531  0.45426187 ... 0.6437231  0.94655716 0.53181237]]\n",
      "\n",
      " [[0.51977044 0.6124255  0.04410373 ... 0.14944035 0.29738176 0.42086688]\n",
      "  [0.02331264 0.06788605 0.77830243 ... 0.83413535 0.08700421 0.7267088 ]\n",
      "  [0.04104438 0.6400285  0.34719583 ... 0.7246397  0.823761   0.558414  ]\n",
      "  ...\n",
      "  [0.597084   0.48698038 0.06504077 ... 0.87367594 0.28494206 0.70533794]\n",
      "  [0.25869054 0.06908277 0.9689559  ... 0.7227215  0.27238044 0.10473777]\n",
      "  [0.97081715 0.8583806  0.34349772 ... 0.5373928  0.89081556 0.78511953]]\n",
      "\n",
      " [[0.80206525 0.43210518 0.9051501  ... 0.9667514  0.48719826 0.17716876]\n",
      "  [0.51766795 0.12923057 0.66509604 ... 0.33673972 0.7506867  0.6384207 ]\n",
      "  [0.5887405  0.1001545  0.37171704 ... 0.31470174 0.25662017 0.34085345]\n",
      "  ...\n",
      "  [0.61200464 0.9427337  0.83827955 ... 0.1811164  0.9560928  0.33478191]\n",
      "  [0.8628206  0.33729008 0.5021494  ... 0.8538563  0.10517444 0.4068367 ]\n",
      "  [0.8190816  0.17954852 0.16535619 ... 0.65822357 0.9057585  0.2691383 ]]\n",
      "\n",
      " [[0.12910023 0.6075177  0.48562065 ... 0.72893447 0.3252463  0.1206186 ]\n",
      "  [0.9793297  0.14342062 0.25164998 ... 0.9232033  0.26819578 0.83665884]\n",
      "  [0.4966428  0.9220257  0.84569204 ... 0.12807241 0.9018147  0.99092895]\n",
      "  ...\n",
      "  [0.33214068 0.8000753  0.5698696  ... 0.53784066 0.2420261  0.1935943 ]\n",
      "  [0.36863807 0.2914943  0.90292704 ... 0.8657944  0.8453209  0.759248  ]\n",
      "  [0.44827947 0.24384844 0.5276361  ... 0.13163114 0.33739206 0.34666717]]\n",
      "\n",
      " [[0.15135363 0.5149915  0.31344974 ... 0.2006888  0.3985358  0.13285826]\n",
      "  [0.12138645 0.3819667  0.6063708  ... 0.54992825 0.03115779 0.39054856]\n",
      "  [0.5049559  0.15799424 0.34675494 ... 0.3847348  0.4483366  0.96588206]\n",
      "  ...\n",
      "  [0.6101116  0.9788105  0.3769062  ... 0.1500812  0.2799933  0.6022521 ]\n",
      "  [0.7635792  0.8501212  0.7064501  ... 0.30245128 0.3272917  0.28746945]\n",
      "  [0.02519213 0.884103   0.60757047 ... 0.5182805  0.9473891  0.72907186]]]\n",
      "(1000, 1) [[0.01543592]\n",
      " [0.7557631 ]\n",
      " [0.1052155 ]\n",
      " [0.6496918 ]\n",
      " [0.24089135]]\n",
      "(1000, 1) [[0.1231597 ]\n",
      " [0.08905373]\n",
      " [0.08373296]\n",
      " [0.04768773]\n",
      " [0.08624092]]\n"
     ]
    }
   ],
   "source": [
    "#Generate some Dummy data to test out the model using\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "embedding_dim = 256\n",
    "encodings = np.random.rand(1000, 10, embedding_dim).astype(np.float32)\n",
    "print(encodings.shape,encodings[:5])\n",
    "\n",
    "sentiments = np.random.rand(1000, 1).astype(np.float32)\n",
    "print(sentiments.shape, sentiments[:5])\n",
    "\n",
    "price_percentage_changes = np.random.rand(1000, 1).astype(np.float32)* 0.2;\n",
    "print(price_percentage_changes.shape, price_percentage_changes[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "2fe291d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "encodings",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "sentiments",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "price_percentage_changes",
         "rawType": "object",
         "type": "unknown"
        }
       ],
       "ref": "33305c85-079d-477c-8f77-43c22899e529",
       "rows": [
        [
         "0",
         "[[0.70924795 0.32848698 0.82711387 ... 0.49807447 0.90072435 0.8727512 ]\n [0.9046867  0.60809374 0.53850275 ... 0.6635218  0.29242167 0.12887849]\n [0.5412136  0.41911846 0.6955833  ... 0.5759633  0.15471639 0.8632402 ]\n ...\n [0.9149834  0.848888   0.8758864  ... 0.23433454 0.3700483  0.7704213 ]\n [0.6062975  0.66818774 0.49246314 ... 0.90030676 0.47267076 0.05881489]\n [0.10701042 0.9551531  0.45426187 ... 0.6437231  0.94655716 0.53181237]]",
         "[0.01543592]",
         "[0.1231597]"
        ],
        [
         "1",
         "[[0.51977044 0.6124255  0.04410373 ... 0.14944035 0.29738176 0.42086688]\n [0.02331264 0.06788605 0.77830243 ... 0.83413535 0.08700421 0.7267088 ]\n [0.04104438 0.6400285  0.34719583 ... 0.7246397  0.823761   0.558414  ]\n ...\n [0.597084   0.48698038 0.06504077 ... 0.87367594 0.28494206 0.70533794]\n [0.25869054 0.06908277 0.9689559  ... 0.7227215  0.27238044 0.10473777]\n [0.97081715 0.8583806  0.34349772 ... 0.5373928  0.89081556 0.78511953]]",
         "[0.7557631]",
         "[0.08905373]"
        ],
        [
         "2",
         "[[0.80206525 0.43210518 0.9051501  ... 0.9667514  0.48719826 0.17716876]\n [0.51766795 0.12923057 0.66509604 ... 0.33673972 0.7506867  0.6384207 ]\n [0.5887405  0.1001545  0.37171704 ... 0.31470174 0.25662017 0.34085345]\n ...\n [0.61200464 0.9427337  0.83827955 ... 0.1811164  0.9560928  0.33478191]\n [0.8628206  0.33729008 0.5021494  ... 0.8538563  0.10517444 0.4068367 ]\n [0.8190816  0.17954852 0.16535619 ... 0.65822357 0.9057585  0.2691383 ]]",
         "[0.1052155]",
         "[0.08373296]"
        ],
        [
         "3",
         "[[0.12910023 0.6075177  0.48562065 ... 0.72893447 0.3252463  0.1206186 ]\n [0.9793297  0.14342062 0.25164998 ... 0.9232033  0.26819578 0.83665884]\n [0.4966428  0.9220257  0.84569204 ... 0.12807241 0.9018147  0.99092895]\n ...\n [0.33214068 0.8000753  0.5698696  ... 0.53784066 0.2420261  0.1935943 ]\n [0.36863807 0.2914943  0.90292704 ... 0.8657944  0.8453209  0.759248  ]\n [0.44827947 0.24384844 0.5276361  ... 0.13163114 0.33739206 0.34666717]]",
         "[0.6496918]",
         "[0.04768773]"
        ],
        [
         "4",
         "[[0.15135363 0.5149915  0.31344974 ... 0.2006888  0.3985358  0.13285826]\n [0.12138645 0.3819667  0.6063708  ... 0.54992825 0.03115779 0.39054856]\n [0.5049559  0.15799424 0.34675494 ... 0.3847348  0.4483366  0.96588206]\n ...\n [0.6101116  0.9788105  0.3769062  ... 0.1500812  0.2799933  0.6022521 ]\n [0.7635792  0.8501212  0.7064501  ... 0.30245128 0.3272917  0.28746945]\n [0.02519213 0.884103   0.60757047 ... 0.5182805  0.9473891  0.72907186]]",
         "[0.24089135]",
         "[0.08624092]"
        ]
       ],
       "shape": {
        "columns": 3,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>encodings</th>\n",
       "      <th>sentiments</th>\n",
       "      <th>price_percentage_changes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[[0.70924795, 0.32848698, 0.82711387, 0.715978...</td>\n",
       "      <td>[0.015435915]</td>\n",
       "      <td>[0.1231597]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[[0.51977044, 0.6124255, 0.044103727, 0.079766...</td>\n",
       "      <td>[0.7557631]</td>\n",
       "      <td>[0.08905373]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[[0.80206525, 0.43210518, 0.9051501, 0.7511358...</td>\n",
       "      <td>[0.105215505]</td>\n",
       "      <td>[0.08373296]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[[0.12910023, 0.6075177, 0.48562065, 0.452957,...</td>\n",
       "      <td>[0.6496918]</td>\n",
       "      <td>[0.04768773]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[[0.15135363, 0.5149915, 0.31344974, 0.5500606...</td>\n",
       "      <td>[0.24089135]</td>\n",
       "      <td>[0.086240925]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           encodings     sentiments  \\\n",
       "0  [[0.70924795, 0.32848698, 0.82711387, 0.715978...  [0.015435915]   \n",
       "1  [[0.51977044, 0.6124255, 0.044103727, 0.079766...    [0.7557631]   \n",
       "2  [[0.80206525, 0.43210518, 0.9051501, 0.7511358...  [0.105215505]   \n",
       "3  [[0.12910023, 0.6075177, 0.48562065, 0.452957,...    [0.6496918]   \n",
       "4  [[0.15135363, 0.5149915, 0.31344974, 0.5500606...   [0.24089135]   \n",
       "\n",
       "  price_percentage_changes  \n",
       "0              [0.1231597]  \n",
       "1             [0.08905373]  \n",
       "2             [0.08373296]  \n",
       "3             [0.04768773]  \n",
       "4            [0.086240925]  "
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_df = pd.DataFrame({\n",
    "    'encodings': list(encodings),\n",
    "    'sentiments': list(sentiments),\n",
    "    'price_percentage_changes': list(price_percentage_changes)\n",
    "})    \n",
    "dummy_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "5007142f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator(batch_size=32):\n",
    "    for _, row in dummy_df.iterrows():\n",
    "        input = row['encodings'] * row['sentiments'][0]\n",
    "        output = torch.tensor(row['price_percentage_changes'], dtype=torch.float32).unsqueeze(0)\n",
    "        yield torch.tensor(input, dtype=torch.float32), torch.tensor(output, dtype=torch.float32)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(dummy_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "c232cf64",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing the data generator\n",
    "generator_object = data_generator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "15962359",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0109, 0.0051, 0.0128,  ..., 0.0077, 0.0139, 0.0135],\n",
      "        [0.0140, 0.0094, 0.0083,  ..., 0.0102, 0.0045, 0.0020],\n",
      "        [0.0084, 0.0065, 0.0107,  ..., 0.0089, 0.0024, 0.0133],\n",
      "        ...,\n",
      "        [0.0141, 0.0131, 0.0135,  ..., 0.0036, 0.0057, 0.0119],\n",
      "        [0.0094, 0.0103, 0.0076,  ..., 0.0139, 0.0073, 0.0009],\n",
      "        [0.0017, 0.0147, 0.0070,  ..., 0.0099, 0.0146, 0.0082]]) tensor([[0.1232]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adith\\AppData\\Local\\Temp\\ipykernel_14828\\3946322897.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  yield torch.tensor(input, dtype=torch.float32), torch.tensor(output, dtype=torch.float32)\n"
     ]
    }
   ],
   "source": [
    "x, y = generator_object.__next__()\n",
    "print(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "a25bb370",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SetTransformer(\n",
    "    dim_input = embedding_dim, \n",
    "    num_outputs = 1, #One final prediction\n",
    "    dim_output = 1, #1D output for price change\n",
    "    num_inds=32, \n",
    "    dim_hidden=10, \n",
    "    num_heads=4, \n",
    "    ln=True #Layer normalization\n",
    "    )\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.03)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "model = nn.DataParallel(model)\n",
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "a68c5b43",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adith\\AppData\\Local\\Temp\\ipykernel_14828\\3946322897.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  yield torch.tensor(input, dtype=torch.float32), torch.tensor(output, dtype=torch.float32)\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "Dimension out of range (expected to be in range of [-2, 1], but got 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[83], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m inps \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor(inps)\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[0;32m      7\u001b[0m outs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor(outs)\u001b[38;5;241m.\u001b[39mlong()\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[1;32m----> 8\u001b[0m preds \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(preds, outs)\n\u001b[0;32m     11\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[1;32mc:\\Users\\adith\\anaconda3\\envs\\MLEnvt_Python38\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\adith\\anaconda3\\envs\\MLEnvt_Python38\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\adith\\anaconda3\\envs\\MLEnvt_Python38\\lib\\site-packages\\torch\\nn\\parallel\\data_parallel.py:183\u001b[0m, in \u001b[0;36mDataParallel.forward\u001b[1;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[0;32m    180\u001b[0m     module_kwargs \u001b[38;5;241m=\u001b[39m ({},)\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice_ids) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 183\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodule_kwargs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    184\u001b[0m replicas \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreplicate(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice_ids[:\u001b[38;5;28mlen\u001b[39m(inputs)])\n\u001b[0;32m    185\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparallel_apply(replicas, inputs, module_kwargs)\n",
      "File \u001b[1;32mc:\\Users\\adith\\anaconda3\\envs\\MLEnvt_Python38\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\adith\\anaconda3\\envs\\MLEnvt_Python38\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[2], line 42\u001b[0m, in \u001b[0;36mSetTransformer.forward\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[1;32m---> 42\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdec(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32mc:\\Users\\adith\\anaconda3\\envs\\MLEnvt_Python38\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\adith\\anaconda3\\envs\\MLEnvt_Python38\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\adith\\anaconda3\\envs\\MLEnvt_Python38\\lib\\site-packages\\torch\\nn\\modules\\container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\adith\\anaconda3\\envs\\MLEnvt_Python38\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\adith\\anaconda3\\envs\\MLEnvt_Python38\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[1], line 52\u001b[0m, in \u001b[0;36mISAB.forward\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[1;32m---> 52\u001b[0m     H \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmab0\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mI\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrepeat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     53\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmab1(X, H)\n",
      "File \u001b[1;32mc:\\Users\\adith\\anaconda3\\envs\\MLEnvt_Python38\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\adith\\anaconda3\\envs\\MLEnvt_Python38\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[1], line 25\u001b[0m, in \u001b[0;36mMAB.forward\u001b[1;34m(self, Q, K)\u001b[0m\n\u001b[0;32m     23\u001b[0m dim_split \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdim_V \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads\n\u001b[0;32m     24\u001b[0m Q_ \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(Q\u001b[38;5;241m.\u001b[39msplit(dim_split, \u001b[38;5;241m2\u001b[39m), \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m---> 25\u001b[0m K_ \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(\u001b[43mK\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdim_split\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m, \u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     26\u001b[0m V_ \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(V\u001b[38;5;241m.\u001b[39msplit(dim_split, \u001b[38;5;241m2\u001b[39m), \u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     28\u001b[0m A \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msoftmax(Q_\u001b[38;5;241m.\u001b[39mbmm(K_\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m2\u001b[39m))\u001b[38;5;241m/\u001b[39mmath\u001b[38;5;241m.\u001b[39msqrt(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdim_V), \u001b[38;5;241m2\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\adith\\anaconda3\\envs\\MLEnvt_Python38\\lib\\site-packages\\torch\\_tensor.py:919\u001b[0m, in \u001b[0;36mTensor.split\u001b[1;34m(self, split_size, dim)\u001b[0m\n\u001b[0;32m    916\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m    918\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(split_size, (\u001b[38;5;28mint\u001b[39m, torch\u001b[38;5;241m.\u001b[39mSymInt)):\n\u001b[1;32m--> 919\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msplit_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m    920\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    921\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_VF\u001b[38;5;241m.\u001b[39msplit_with_sizes(\u001b[38;5;28mself\u001b[39m, split_size, dim)\n",
      "\u001b[1;31mIndexError\u001b[0m: Dimension out of range (expected to be in range of [-2, 1], but got 2)"
     ]
    }
   ],
   "source": [
    "num_epochs = 20\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    losses, total, correct = [], 0, 0\n",
    "    for inps,outs in data_generator():\n",
    "        inps = torch.Tensor(inps).cuda()\n",
    "        outs = torch.Tensor(outs).long().cuda()\n",
    "        preds = model(inps)\n",
    "        loss = criterion(preds, outs)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        losses.append(loss.item())\n",
    "        total += outs.shape[0]\n",
    "        correct += (preds.argmax(dim=1) == outs).sum().item()\n",
    "\n",
    "    avg_loss, avg_acc = np.mean(losses), correct / total\n",
    "    print(f\"Epoch {epoch}: train loss {avg_loss:.3f} train acc {avg_acc:.3f}\")\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(inps)\n",
    "        print(outs)\n",
    "        model.eval()\n",
    "        losses, total, correct = [], 0, 0\n",
    "        for inps, outs in data_generator():\n",
    "            inps = torch.Tensor(inps).cuda()\n",
    "            outs = torch.Tensor(outs).long().cuda()\n",
    "            preds = model(inps)\n",
    "            loss = criterion(preds, outs)\n",
    "\n",
    "            losses.append(loss.item())\n",
    "            total += outs.shape[0]\n",
    "            correct += (preds.argmax(dim=1) == outs).sum().item()\n",
    "        avg_loss, avg_acc = np.mean(losses), correct / total\n",
    "        print(f\"Epoch {epoch}: test loss {avg_loss:.3f} test acc {avg_acc:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe8f6dae",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLEnvt_Python38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
