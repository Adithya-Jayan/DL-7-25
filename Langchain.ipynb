{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c485d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import copy\n",
    "# -------------------------------------------------------------------------\n",
    "#  LangChain Imports\n",
    "# -------------------------------------------------------------------------\n",
    "from langchain.chains import SequentialChain, LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import OpenAI  # Replace with any LLM provider\n",
    "from langchain.output_parsers import RegexParser\n",
    "# -------------------------------------------------------------------------\n",
    "#  Web Scraping Imports\n",
    "# -------------------------------------------------------------------------\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "from datetime import timedelta\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "# -------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "16061cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_latest_bullionvault_articles(URL=\"https://www.bullionvault.com/gold-news\"):\n",
    "    response = requests.get(URL)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    latest=soup.find(id='views-bootstrap-grid-1').find_all(class_='field-content')\n",
    "    list_data = []\n",
    "    for item in latest:\n",
    "        date=item.find(class_='views-field-created')\n",
    "        if not date:\n",
    "            continue\n",
    "        link=item.find(class_='views-field-title').find('a')['href']\n",
    "        page_response = requests.get(link)\n",
    "        page_soup = BeautifulSoup(page_response.content, 'html.parser')\n",
    "        content = page_soup.find('div', class_='field field-name-body field-type-text-with-summary field-label-hidden')\n",
    "        title = page_soup.find('h1').text.strip()\n",
    "        content_text = content.text.strip() if content else ''\n",
    "        data_point = {'Date': date.text.strip() if date else 'N/A', 'Content': title + ':' + content_text}\n",
    "        list_data.append(data_point)\n",
    "    list_df=pd.DataFrame(list_data)\n",
    "    list_df['Date']= pd.to_datetime(list_df['Date'],errors='coerce').dt.date\n",
    "    return list_df\n",
    "\n",
    "def yf_extract_info(item):\n",
    "    link=item.find('a',class_='subtle-link')['href']\n",
    "    title=item.find('a',class_='subtle-link')['title']\n",
    "\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--headless\")  # Run in headless mode\n",
    "    chrome_options.add_argument(\"--disable-gpu\")  # Disable GPU acceleration\n",
    "\n",
    "    page_driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=chrome_options)\n",
    "    page_driver.get(link)\n",
    "\n",
    "    page_soup = BeautifulSoup(page_driver.page_source, 'html.parser')\n",
    "    content = page_soup.find('div', class_='body')\n",
    "    content_text = content.text.strip() if content else ''\n",
    "    date= page_soup.find('div', class_= lambda c: c and c.startswith(\"byline\")).find('time')\n",
    "    data_point = {'Date': date.text.strip() if date else 'N/A', 'Content': title + ':' + content_text}\n",
    "    page_driver.quit()\n",
    "    return data_point\n",
    "\n",
    "def get_latest_yf_articles(URL=\"https://finance.yahoo.com/news/\"):\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--headless\")  # Run in headless mode\n",
    "    chrome_options.add_argument(\"--disable-gpu\")  # Disable GPU acceleration\n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=chrome_options)\n",
    "    driver.get(URL)\n",
    "    time.sleep(2)\n",
    "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    count = 0\n",
    "    while True:\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(2)  # Wait for new content to load\n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        if new_height == last_height:\n",
    "            break\n",
    "        last_height = new_height\n",
    "        count += 1\n",
    "        if count > 1:\n",
    "            break\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    articles = soup.find_all('li', class_='story-item')\n",
    "    list_data = []\n",
    "    #print(f\"Found {len(articles)} articles on Yahoo Finance.\")\n",
    "    for article in articles:\n",
    "        try:\n",
    "            list_data.append(yf_extract_info(article))\n",
    "        except Exception as e:\n",
    "            continue\n",
    "    driver.quit()\n",
    "    list_df=pd.DataFrame(list_data)\n",
    "    list_df['Date'].dropna(inplace=True)\n",
    "    #list_df['Date']= pd.to_datetime(list_df['Date'],errors='coerce').dt.date\n",
    "    return list_df\n",
    "\n",
    "def get_reuters_article_text(item, base_URL=\"https://www.reuters.com\"):\n",
    "    title = item.get_text(strip=True)\n",
    "    link = item.find('a', href=True)['href']\n",
    "    if not link.startswith('http'):\n",
    "        link = base_URL + link\n",
    "    chrome_options = Options()\n",
    "    driver = webdriver.Chrome(options=chrome_options)\n",
    "    driver.get(link)\n",
    "    time.sleep(2)\n",
    "    page_soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    driver.quit()\n",
    "    article = page_soup.find_all(\"div\", class_=lambda c: c and c.startswith(\"article-body__paragraph\"))\n",
    "    article_text = \"\"\n",
    "    for para in article:\n",
    "        paragraph_text = para.get_text(strip=True)\n",
    "        article_text = article_text + \".\" + paragraph_text\n",
    "    date = page_soup.find(\"span\", class_=lambda c: c and c.startswith(\"date-line__date\")).get_text(strip=True)\n",
    "    data_point = {'Date': date, 'Content': title + ':' + article_text}\n",
    "    return data_point\n",
    "\n",
    "def get_reuters_articles_list(URL):\n",
    "    chrome_options = Options()\n",
    "    driver = webdriver.Chrome(options=chrome_options)\n",
    "    driver.get(URL)\n",
    "    time.sleep(2)\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    driver.quit()\n",
    "    articles=soup.find_all(\"div\", class_=lambda c: c and c.startswith(\"story-card__area-headline\"))\n",
    "    list_data = []\n",
    "    for article in articles:\n",
    "        try:\n",
    "            list_data.append(get_reuters_article_text(article))\n",
    "        except Exception as e:\n",
    "            continue\n",
    "    list_df=pd.DataFrame(list_data)\n",
    "    list_df['Date']= pd.to_datetime(list_df['Date'],errors='coerce').dt.date\n",
    "    return list_df\n",
    "\n",
    "def get_reuters_articles():\n",
    "    base_URL=\"https://www.reuters.com\"\n",
    "    search_query=\"/site-search/?query=gold\"\n",
    "    df = pd.DataFrame(columns=['Date', 'Content'])\n",
    "    for section_val in ['all']:\n",
    "        for offset_nb in range(0, 40, 20):\n",
    "            offset =f\"&offset={offset_nb}\"\n",
    "            section=f\"&section={section_val}\"\n",
    "            URL = base_URL + search_query + offset + section\n",
    "            try:\n",
    "                df_latest=get_reuters_articles_list(URL)\n",
    "                df = pd.concat([df, df_latest], ignore_index=True)\n",
    "            except Exception as e:\n",
    "                print(f\"Error fetching articles from {URL}: {e}\")\n",
    "                continue\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a4c666",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load latest news data from various sources\n",
    "def extract_news_data():\n",
    "    bullion_df = get_latest_bullionvault_articles()\n",
    "    yf_df=get_latest_yf_articles()\n",
    "    yf_df['Date']=pd.to_datetime(yf_df['Date'],errors='coerce').dt.date\n",
    "    reuters_df = get_reuters_articles()\n",
    "    three_days_ago = pd.to_datetime('today').date() - timedelta(days=3)\n",
    "\n",
    "    df_combined = pd.concat([bullion_df, yf_df, reuters_df], ignore_index=True)\n",
    "    df_combined = df_combined.sort_values(by='Date')\n",
    "    df_combined=df_combined[df_combined['Date'] >= three_days_ago]\n",
    "    return df_combined"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
