{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c485d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import copy\n",
    "#!pip install beautifulsoup4 pandas selenium webdriver_manager\n",
    "# -------------------------------------------------------------------------\n",
    "#  LangChain Imports\n",
    "# -------------------------------------------------------------------------\n",
    "from langchain.chains import SequentialChain, LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import OpenAI  # Replace with any LLM provider\n",
    "from langchain.output_parsers import RegexParser\n",
    "# -------------------------------------------------------------------------\n",
    "#  Web Scraping Imports\n",
    "# -------------------------------------------------------------------------\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "from datetime import timedelta\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "# -------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "16061cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_latest_bullionvault_articles(URL=\"https://www.bullionvault.com/gold-news\"):\n",
    "    response = requests.get(URL)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    latest=soup.find(id='views-bootstrap-grid-1').find_all(class_='field-content')\n",
    "    list_data = []\n",
    "    for item in latest:\n",
    "        date=item.find(class_='views-field-created')\n",
    "        if not date:\n",
    "            continue\n",
    "        link=item.find(class_='views-field-title').find('a')['href']\n",
    "        page_response = requests.get(link)\n",
    "        page_soup = BeautifulSoup(page_response.content, 'html.parser')\n",
    "        content = page_soup.find('div', class_='field field-name-body field-type-text-with-summary field-label-hidden')\n",
    "        title = page_soup.find('h1').text.strip()\n",
    "        content_text = content.text.strip() if content else ''\n",
    "        data_point = {'Date': date.text.strip() if date else 'N/A', 'Content': title + ':' + content_text}\n",
    "        list_data.append(data_point)\n",
    "    list_df=pd.DataFrame(list_data)\n",
    "    list_df['Date']= pd.to_datetime(list_df['Date'],errors='coerce').dt.date\n",
    "    return list_df\n",
    "\n",
    "def yf_extract_info(item):\n",
    "    link=item.find('a',class_='subtle-link')['href']\n",
    "    title=item.find('a',class_='subtle-link')['title']\n",
    "\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--headless\")  # Run in headless mode\n",
    "    chrome_options.add_argument(\"--disable-gpu\")  # Disable GPU acceleration\n",
    "\n",
    "    page_driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=chrome_options)\n",
    "    page_driver.get(link)\n",
    "\n",
    "    page_soup = BeautifulSoup(page_driver.page_source, 'html.parser')\n",
    "    content = page_soup.find('div', class_='body')\n",
    "    content_text = content.text.strip() if content else ''\n",
    "    date= page_soup.find('div', class_= lambda c: c and c.startswith(\"byline\")).find('time')\n",
    "    data_point = {'Date': date.text.strip() if date else 'N/A', 'Content': title + ':' + content_text}\n",
    "    page_driver.quit()\n",
    "    return data_point\n",
    "\n",
    "def get_latest_yf_articles(URL=\"https://finance.yahoo.com/news/\"):\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--headless\")  # Run in headless mode\n",
    "    chrome_options.add_argument(\"--disable-gpu\")  # Disable GPU acceleration\n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=chrome_options)\n",
    "    driver.get(URL)\n",
    "    time.sleep(2)\n",
    "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    count = 0\n",
    "    while True:\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(2)  # Wait for new content to load\n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        if new_height == last_height:\n",
    "            break\n",
    "        last_height = new_height\n",
    "        count += 1\n",
    "        if count > 1:\n",
    "            break\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    articles = soup.find_all('li', class_='story-item')\n",
    "    list_data = []\n",
    "    #print(f\"Found {len(articles)} articles on Yahoo Finance.\")\n",
    "    for article in articles:\n",
    "        try:\n",
    "            list_data.append(yf_extract_info(article))\n",
    "        except Exception as e:\n",
    "            continue\n",
    "    driver.quit()\n",
    "    list_df=pd.DataFrame(list_data)\n",
    "    list_df['Date'].dropna(inplace=True)\n",
    "    #list_df['Date']= pd.to_datetime(list_df['Date'],errors='coerce').dt.date\n",
    "    return list_df\n",
    "\n",
    "def get_reuters_article_text(item, base_URL=\"https://www.reuters.com\"):\n",
    "    title = item.get_text(strip=True)\n",
    "    link = item.find('a', href=True)['href']\n",
    "    if not link.startswith('http'):\n",
    "        link = base_URL + link\n",
    "    chrome_options = Options()\n",
    "    driver = webdriver.Chrome(options=chrome_options)\n",
    "    driver.get(link)\n",
    "    time.sleep(2)\n",
    "    page_soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    driver.quit()\n",
    "    article = page_soup.find_all(\"div\", class_=lambda c: c and c.startswith(\"article-body__paragraph\"))\n",
    "    article_text = \"\"\n",
    "    for para in article:\n",
    "        paragraph_text = para.get_text(strip=True)\n",
    "        article_text = article_text + \".\" + paragraph_text\n",
    "    date = page_soup.find(\"span\", class_=lambda c: c and c.startswith(\"date-line__date\")).get_text(strip=True)\n",
    "    data_point = {'Date': date, 'Content': title + ':' + article_text}\n",
    "    return data_point\n",
    "\n",
    "def get_reuters_articles_list(URL):\n",
    "    chrome_options = Options()\n",
    "    driver = webdriver.Chrome(options=chrome_options)\n",
    "    driver.get(URL)\n",
    "    time.sleep(2)\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    driver.quit()\n",
    "    articles=soup.find_all(\"div\", class_=lambda c: c and c.startswith(\"story-card__area-headline\"))\n",
    "    list_data = []\n",
    "    for article in articles:\n",
    "        try:\n",
    "            list_data.append(get_reuters_article_text(article))\n",
    "        except Exception as e:\n",
    "            continue\n",
    "    list_df=pd.DataFrame(list_data)\n",
    "    list_df['Date']= pd.to_datetime(list_df['Date'],errors='coerce').dt.date\n",
    "    return list_df\n",
    "\n",
    "def get_reuters_articles():\n",
    "    base_URL=\"https://www.reuters.com\"\n",
    "    search_query=\"/site-search/?query=gold\"\n",
    "    df = pd.DataFrame(columns=['Date', 'Content'])\n",
    "    for section_val in ['all']:\n",
    "        for offset_nb in range(0, 40, 20):\n",
    "            offset =f\"&offset={offset_nb}\"\n",
    "            section=f\"&section={section_val}\"\n",
    "            URL = base_URL + search_query + offset + section\n",
    "            try:\n",
    "                df_latest=get_reuters_articles_list(URL)\n",
    "                df = pd.concat([df, df_latest], ignore_index=True)\n",
    "            except Exception as e:\n",
    "                print(f\"Error fetching articles from {URL}: {e}\")\n",
    "                continue\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e4a4c666",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load/clean dummy data (placeholder)\n",
    "def extract_news_data():\n",
    "    bullion_df = get_latest_bullionvault_articles()\n",
    "    yf_df=get_latest_yf_articles()\n",
    "    yf_df['Date']=pd.to_datetime(yf_df['Date'],errors='coerce').dt.date\n",
    "    reuters_df = get_reuters_articles()\n",
    "    three_days_ago = pd.to_datetime('today').date() - timedelta(days=3)\n",
    "\n",
    "    df_combined = pd.concat([bullion_df, yf_df, reuters_df], ignore_index=True)\n",
    "    df_combined = df_combined.sort_values(by='Date')\n",
    "    df_combined=df_combined[df_combined['Date'] >= three_days_ago]\n",
    "    # Placeholder for actual news data extraction logic\n",
    "    return df_combined\n",
    "news_data = \"Oil prices surged due to Middle East tensions.\"\n",
    "price_data = \"RSI indicates strong momentum in gold stocks.\"\n",
    "\n",
    "# Step 2: Sentence Pre-processing\n",
    "news_sentences = [news_data]  # In practice, split into sentences\n",
    "price_sentences = [price_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ed33a0ac",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'yf_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43myf_df\u001b[49m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'yf_df' is not defined"
     ]
    }
   ],
   "source": [
    "yf_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ac0f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Topic & Sentiment Extraction\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"sentence\"],\n",
    "    template=\"Extract the main subject and sentiment score (between -1 to 1) from this sentence:\\n{sentence}\"\n",
    ")\n",
    "llm_chain = LLMChain(llm=OpenAI(temperature=0), prompt=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b438667b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract topic and sentiment using a mock LLM\n",
    "def extract_topic_sentiment(sentences):\n",
    "    # Placeholder: Replace with actual LLM call\n",
    "    # Here, we mock the output for demonstration\n",
    "    if sentences == news_sentences:\n",
    "        return {\"Oil\": 0.92, \"Middle East\": 0.7}\n",
    "    else:\n",
    "        return {\"Gold\": 0.85, \"RSI\": 0.6}\n",
    "\n",
    "# Step 4: Generate dictionary of {topic: sentiment} for each path\n",
    "news_topic_sentiment = extract_topic_sentiment(news_sentences)\n",
    "price_topic_sentiment = extract_topic_sentiment(price_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be7a5b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Generate dictionary of {topic: sentiment}\n",
    "results = [topic_sentiment_chain.run(sentence=s) for s in sentences]\n",
    "topic_sentiment_dict = {\n",
    "    \"Oil\": 0.92,\n",
    "    \"Harvest\": 0.02,\n",
    "    \"GDP\": 0.45,\n",
    "    \"Boxoffice\": 0.58\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb852c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Custom Model Placeholder (simulate transformation)\n",
    "def custom_model_output(ts_dict):\n",
    "    return {topic: score * 1.1 for topic, score in ts_dict.items()}\n",
    "\n",
    "# Step 6: Base Model Placeholder (simulate transformation)\n",
    "def base_model_output(ts_dict):\n",
    "    return {topic: score * 0.95 for topic, score in ts_dict.items()}\n",
    "\n",
    "# Step 7: Meta-Model using Ensemble Strategy (simple averaging)\n",
    "def meta_model(custom_out, base_out):\n",
    "    ensemble = {}\n",
    "    for k in custom_out:\n",
    "        ensemble[k] = (custom_out[k] + base_out.get(k, 0)) / 2\n",
    "    return ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc32d6ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- News Path ---\n",
    "news_custom = custom_model_output(news_topic_sentiment)\n",
    "news_base = base_model_output(news_topic_sentiment)\n",
    "news_ensemble = meta_model(news_custom, news_base)\n",
    "\n",
    "# --- Price Path ---\n",
    "price_custom = custom_model_output(price_topic_sentiment)\n",
    "price_base = base_model_output(price_topic_sentiment)\n",
    "price_ensemble = meta_model(price_custom, price_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee1ea4b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Final Meta-Model: Combine News and Price Ensembles ---\n",
    "final_input = {**news_ensemble, **price_ensemble}  # Merge both\n",
    "# Optionally, you could run another meta_model or further logic here\n",
    "\n",
    "print(\"News Ensemble Output:\", news_ensemble)\n",
    "print(\"Price Ensemble Output:\", price_ensemble)\n",
    "print(\"Final Combined Output:\", final_input)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
