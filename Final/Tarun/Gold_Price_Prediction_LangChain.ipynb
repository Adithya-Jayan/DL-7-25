{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c8d18d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This script installs all required libraries for data analysis, plotting, LLM workflows, and notebook imports.\n",
    "# Note: The installation command is commented out to prevent accidental execution.\n",
    "# --------------------------------------------------------------------------------\n",
    "\n",
    "# Required Libraries:\n",
    "# pandas: Data manipulation and analysis\n",
    "# numpy: Numerical computations\n",
    "# matplotlib: Data visualization\n",
    "# yfinance: Downloading financial data from Yahoo Finance\n",
    "# langchain: Building LLM-powered applications and chains\n",
    "# import_ipynb: Importing Jupyter notebooks as Python modules\n",
    "# scipy: Scientific computing (e.g., signal processing)\n",
    "# statsmodels: Statistical modeling and time series analysis\n",
    "# xgboost: Gradient boosting for machine learning\n",
    "# selenium: Web scraping and browser automation\n",
    "# webdriver_manager: Managing browser drivers for Selenium\n",
    "# transformers: State-of-the-art NLP models\n",
    "# peft: Parameter-efficient fine-tuning for transformers\n",
    "# accelerate: Optimizing training and inference of models\n",
    "# bitsandbytes: Efficient training of large models with 8-bit optimizers\n",
    "# tensorflow: Deep learning framework\n",
    "# torch: PyTorch deep learning framework\n",
    "# tensorboard: Visualization tool for TensorFlow and PyTorch\n",
    "# scikit-learn: Machine learning library for Python (version 1.6.1)\n",
    "\n",
    "# Install all required libraries\n",
    "#%pip install -U tensorflow pandas torch tensorboard numpy matplotlib yfinance langchain import_ipynb scipy statsmodels xgboost selenium webdriver_manager transformers peft accelerate bitsandbytes\n",
    "#%pip install scikit-learn==1.6.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb411c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# -------------------------------------------------------------------------\n",
    "#  LangChain Imports\n",
    "# -------------------------------------------------------------------------\n",
    "import datetime\n",
    "#from langchain.chains import SequentialChain, LLMChain\n",
    "#from langchain.prompts import PromptTemplate\n",
    "#from langchain.llms import OpenAI  # Replace with any LLM provider\n",
    "#from langchain.output_parsers import RegexParser\n",
    "# -------------------------------------------------------------------------\n",
    "# Other Imports\n",
    "# -------------------------------------------------------------------------\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import RobustScaler, MinMaxScaler\n",
    "from datetime import datetime, timedelta\n",
    "from torch.utils.data import Dataset\n",
    "import statsmodels.api as sm\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "# -------------------------------------------------------------------------\n",
    "#  Custom Imports\n",
    "from modules.modules import SetTransformer, VariableSetDataset\n",
    "from modules.functions import *\n",
    "# -------------------------------------------------------------------------\n",
    "#  Web Scraping Imports\n",
    "# -------------------------------------------------------------------------\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "from datetime import timedelta\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "# -------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ce4ae936",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GoldPriceDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size=64, num_layers=2):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)\n",
    "        return self.fc(out[:, -1, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "63ca9cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_checkpoint(filepath, model, device='cpu'):\n",
    "    checkpoint = torch.load(filepath, map_location=device, weights_only=False)  # Set weights_only=False\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    epoch = checkpoint['epoch']\n",
    "    loss = checkpoint['loss']\n",
    "    return epoch, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c83329e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_next_day_gold_price_arimax(df: pd.DataFrame, model) -> float:\n",
    "    \"\"\"\n",
    "    Predict next day's gold price using ARIMAX with technical indicators.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (next_day_price)\n",
    "    \"\"\"\n",
    "    exog_cols = [\n",
    "        'Returns', 'MA_5', 'MA_20', 'MA_50', 'Volatility',\n",
    "        'RSI', 'BB_upper', 'BB_lower', 'BB_width',\n",
    "        'BB_position', 'Sentiment',\n",
    "        'MACD', 'MACD_Signal', 'MACD_Hist',\n",
    "        'Momentum_10', 'ROC_10'\n",
    "    ]\n",
    "\n",
    "    for col in exog_cols:\n",
    "        if col not in df.columns:\n",
    "            raise ValueError(f\"Missing required column: {col}\")\n",
    "\n",
    "    df = df[['Close'] + exog_cols].dropna()\n",
    "    df = df.asfreq('B')\n",
    "    df.ffill(inplace=True)\n",
    "\n",
    "    y = df['Close']\n",
    "    exog = df[exog_cols]\n",
    "\n",
    "    # -------------------------------\n",
    "    # Forecast Next Price\n",
    "    # -------------------------------\n",
    "    next_exog = exog.iloc[[-1]].values\n",
    "    predicted_price = model.forecast(steps=1, exog=next_exog).iloc[0]\n",
    "\n",
    "    return predicted_price\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fbbfe291",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_next_day_gold_price_xgboost(gold: pd.DataFrame, model) -> float:\n",
    "    \"\"\"\n",
    "    Predict next day's gold price using XGBoost with technical indicators.\n",
    "    Returns:\n",
    "        tuple: (next_day_price)\n",
    "    \"\"\"\n",
    "\n",
    "    feature_cols = [\n",
    "        'Returns', 'MA_5', 'MA_20', 'MA_50', 'Volatility',\n",
    "        'RSI', 'BB_upper', 'BB_lower', 'BB_width',\n",
    "        'BB_position', 'Sentiment',\n",
    "        'MACD', 'MACD_Signal', 'MACD_Hist',\n",
    "        'Momentum_10', 'ROC_10'\n",
    "    ]\n",
    "\n",
    "    gold_clean = gold[['Close'] + feature_cols].copy().dropna()\n",
    "    gold_clean['Close_pct_change_1'] = gold_clean['Close'].pct_change(1)\n",
    "    gold_clean['Close_pct_change_2'] = gold_clean['Close'].pct_change(2)\n",
    "    gold_clean['Close_pct_change_3'] = gold_clean['Close'].pct_change(3)\n",
    "    gold_clean['Close_rolling_std_5'] = gold_clean['Close'].rolling(5).std()\n",
    "    gold_clean['Close_rolling_std_10'] = gold_clean['Close'].rolling(10).std()\n",
    "    gold_clean['Close_vs_MA5'] = (gold_clean['Close'] - gold_clean['MA_5']) / gold_clean['MA_5']\n",
    "    gold_clean['Close_vs_MA20'] = (gold_clean['Close'] - gold_clean['MA_20']) / gold_clean['MA_20']\n",
    "    feature_cols_extended = feature_cols + [\n",
    "        'Close_pct_change_1', 'Close_pct_change_2', 'Close_pct_change_3',\n",
    "        'Close_rolling_std_5', 'Close_rolling_std_10',\n",
    "        'Close_vs_MA5', 'Close_vs_MA20']\n",
    "    gold_clean = gold_clean.dropna()\n",
    "    gold_clean['Target_pct_change'] = gold_clean['Close'].pct_change().shift(-1)\n",
    "    gold_clean['Target_price'] = gold_clean['Close'].shift(-1)\n",
    "    gold_clean = gold_clean.dropna()\n",
    "    gold_clean = gold_clean[\n",
    "        (np.isfinite(gold_clean['Target_pct_change'])) &\n",
    "        (np.abs(gold_clean['Target_pct_change']) < 1.0)\n",
    "    ]\n",
    "\n",
    "    # Predict next day\n",
    "    latest_features = gold_clean[feature_cols_extended].iloc[[-1]]\n",
    "    latest_price = gold_clean['Close'].iloc[-1]\n",
    "    scaler = RobustScaler().fit(gold_clean[feature_cols_extended])\n",
    "    latest_scaled = scaler.transform(latest_features)\n",
    "    next_day_pct_change = model.predict(latest_scaled)[0]\n",
    "    next_day_price = latest_price * (1 + next_day_pct_change)\n",
    "\n",
    "    return next_day_price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6e40ca0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_next_day_gold_price_rf(gold: pd.DataFrame, model) -> float:\n",
    "    \"\"\"\n",
    "    Predict next day's gold price using Random Forest with enhanced features.\n",
    "    Saves model daily and loads if already exists. Returns price, model, and percentage change.\n",
    "    \"\"\"\n",
    "\n",
    "    # Feature Engineering\n",
    "    feature_cols = [\n",
    "        'Returns', 'MA_5', 'MA_20', 'MA_50', 'Volatility',\n",
    "        'RSI', 'BB_upper', 'BB_lower', 'BB_width',\n",
    "        'BB_position', 'Sentiment'\n",
    "    ]\n",
    "\n",
    "    gold_clean = gold[['Close'] + feature_cols].copy()\n",
    "    gold_clean = gold_clean.dropna()\n",
    "    gold_clean['Close_pct_change_1'] = gold_clean['Close'].pct_change(1)\n",
    "    gold_clean['Close_pct_change_2'] = gold_clean['Close'].pct_change(2)\n",
    "    gold_clean['Close_pct_change_3'] = gold_clean['Close'].pct_change(3)\n",
    "    gold_clean['Close_rolling_std_5'] = gold_clean['Close'].rolling(5).std()\n",
    "    gold_clean['Close_rolling_std_10'] = gold_clean['Close'].rolling(10).std()\n",
    "    gold_clean['Close_vs_MA5'] = (gold_clean['Close'] - gold_clean['MA_5']) / gold_clean['MA_5']\n",
    "    gold_clean['Close_vs_MA20'] = (gold_clean['Close'] - gold_clean['MA_20']) / gold_clean['MA_20']\n",
    "    gold_clean['Price_momentum_3'] = gold_clean['Close'] / gold_clean['Close'].shift(3) - 1\n",
    "    gold_clean['Price_momentum_5'] = gold_clean['Close'] / gold_clean['Close'].shift(5) - 1\n",
    "\n",
    "    feature_cols_extended = feature_cols + [\n",
    "        'Close_pct_change_1', 'Close_pct_change_2', 'Close_pct_change_3',\n",
    "        'Close_rolling_std_5', 'Close_rolling_std_10',\n",
    "        'Close_vs_MA5', 'Close_vs_MA20',\n",
    "        'Price_momentum_3', 'Price_momentum_5']\n",
    "\n",
    "    gold_clean.dropna(inplace=True)\n",
    "    gold_clean['Target_pct_change'] = gold_clean['Close'].pct_change().shift(-1)\n",
    "    gold_clean['Target_price'] = gold_clean['Close'].shift(-1)\n",
    "    gold_clean.dropna(inplace=True)\n",
    "    gold_clean = gold_clean[(np.abs(gold_clean['Target_pct_change']) < 1.0)]\n",
    "\n",
    "    # Predict next day\n",
    "    latest_features = gold_clean[feature_cols_extended].iloc[[-1]]\n",
    "    latest_price = gold_clean['Close'].iloc[-1]\n",
    "    scaler = RobustScaler().fit(gold_clean[feature_cols_extended])\n",
    "    latest_scaled = scaler.transform(latest_features)\n",
    "    next_day_pct_change = model.predict(latest_scaled)[0]\n",
    "    next_day_price = latest_price * (1 + next_day_pct_change)\n",
    "\n",
    "    return next_day_price\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7a7aafe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_next_day_gold_price_lstm(gold: pd.DataFrame, model=None) -> float:\n",
    "    \n",
    "    sequence_length = 10 ## Based on the model's training sequence length\n",
    "    \n",
    "    # -------------------------------\n",
    "    # Feature Setup\n",
    "    # -------------------------------\n",
    "    feature_cols = [\n",
    "        'Returns', 'MA_5', 'MA_20', 'MA_50', 'Volatility',\n",
    "        'RSI', 'BB_upper', 'BB_lower', 'BB_width',\n",
    "        'BB_position', 'Sentiment'\n",
    "    ]\n",
    "\n",
    "    gold = gold[['Close'] + feature_cols].dropna()\n",
    "    gold = gold.asfreq('B')\n",
    "    gold.ffill(inplace=True)\n",
    "    gold['Target'] = gold['Close'].shift(-1)\n",
    "    gold.dropna(inplace=True)\n",
    "\n",
    "    scaler = MinMaxScaler()\n",
    "    X_scaled = scaler.fit_transform(gold[feature_cols])\n",
    "    y_scaled = scaler.fit_transform(gold[['Target']])\n",
    "\n",
    "    X_seq, y_seq = [], []\n",
    "    for i in range(len(X_scaled) - sequence_length):\n",
    "        X_seq.append(X_scaled[i:i + sequence_length])\n",
    "        y_seq.append(y_scaled[i + sequence_length])\n",
    "\n",
    "    X_seq = np.array(X_seq)\n",
    "    y_seq = np.array(y_seq)\n",
    "\n",
    "    # -------------------------------\n",
    "    # Forecast\n",
    "    # -------------------------------\n",
    "    model.eval()\n",
    "    last_seq = torch.tensor(X_scaled[-sequence_length:], dtype=torch.float32).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        next_pred = model(last_seq).cpu().numpy()\n",
    "\n",
    "    predicted_price = scaler.inverse_transform(\n",
    "        np.concatenate([np.zeros((1, len(feature_cols))), next_pred], axis=1)\n",
    "    )[:, -1][0]\n",
    "\n",
    "    return predicted_price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "32fc48a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_next_day_gold_price_ensemble(\n",
    "    ensemble_model: dict,\n",
    "    arimax_pred: float,\n",
    "    xgb_pred: float,\n",
    "    rf_pred: float,\n",
    "    lstm_pred: float,\n",
    "    llm_pred: float\n",
    "):\n",
    "    \"\"\"\n",
    "    Use a pre-trained ensemble model (results hash) and new predictions to create a results hash.\n",
    "    Only updates the 'individual_predictions' and recalculates all ensemble outputs.\n",
    "    \"\"\"\n",
    "    # Extract weights and metadata from the loaded ensemble model\n",
    "    weights_used = ensemble_model.get('weights_used', {})\n",
    "    meta_weights = weights_used.get('meta_weights', {\n",
    "        'simple': 0.1, 'weighted': 0.25, 'sentiment': 0.2, 'volatility': 0.25, 'trend': 0.2\n",
    "    })\n",
    "    norm_weights = weights_used.get('normalized_weights', {\n",
    "        'arimax': 0.15, 'xgboost': 0.25, 'rf': 0.20, 'lstm': 0.20, 'llm': 0.20\n",
    "    })\n",
    "    vol_weights = weights_used.get('volatility_weights', {\n",
    "        'arimax': 0.25, 'xgboost': 0.20, 'rf': 0.20, 'lstm': 0.15, 'llm': 0.20\n",
    "    })\n",
    "    trend_weights = weights_used.get('trend_weights', {\n",
    "        'arimax': 0.15, 'xgboost': 0.25, 'rf': 0.20, 'lstm': 0.20, 'llm': 0.20\n",
    "    })\n",
    "\n",
    "    # Extract metadata for calculation\n",
    "    metadata = ensemble_model.get('metadata', {})\n",
    "    current_price = metadata.get('current_price', 0)\n",
    "    current_sentiment = metadata.get('current_sentiment', 0)\n",
    "\n",
    "    model_names = ['arimax', 'xgboost', 'rf', 'lstm', 'llm']\n",
    "    model_preds = [arimax_pred, xgb_pred, rf_pred, lstm_pred, llm_pred]\n",
    "\n",
    "    # 1. Simple average\n",
    "    simple_avg = np.mean(model_preds)\n",
    "\n",
    "    # 2. Weighted average (sentiment-boosted)\n",
    "    weighted_avg = sum(norm_weights[k] * p for k, p in zip(model_names, model_preds))\n",
    "\n",
    "    # 3. Sentiment-adjusted\n",
    "    sentiment_factor = 1 + 0.02 * current_sentiment if abs(current_sentiment) > 0.1 else 1.0\n",
    "    sentiment_adjusted = weighted_avg * sentiment_factor\n",
    "\n",
    "    # 4. Volatility-weighted\n",
    "    volatility_weighted = sum(vol_weights[k] * p for k, p in zip(model_names, model_preds))\n",
    "\n",
    "    # 5. Trend-following\n",
    "    trend_following = sum(trend_weights[k] * p for k, p in zip(model_names, model_preds))\n",
    "\n",
    "    # 6. Meta-Ensemble\n",
    "    meta_ensemble = (\n",
    "        meta_weights['simple'] * simple_avg +\n",
    "        meta_weights['weighted'] * weighted_avg +\n",
    "        meta_weights['sentiment'] * sentiment_adjusted +\n",
    "        meta_weights['volatility'] * volatility_weighted +\n",
    "        meta_weights['trend'] * trend_following\n",
    "    )\n",
    "\n",
    "    pct_changes = {\n",
    "        'simple_avg': (simple_avg - current_price) / current_price * 100 if current_price else 0,\n",
    "        'weighted_avg': (weighted_avg - current_price) / current_price * 100 if current_price else 0,\n",
    "        'sentiment_adjusted': (sentiment_adjusted - current_price) / current_price * 100 if current_price else 0,\n",
    "        'volatility_weighted': (volatility_weighted - current_price) / current_price * 100 if current_price else 0,\n",
    "        'trend_following': (trend_following - current_price) / current_price * 100 if current_price else 0,\n",
    "        'meta_ensemble': (meta_ensemble - current_price) / current_price * 100 if current_price else 0\n",
    "    }\n",
    "\n",
    "    results = {\n",
    "        'predictions': {\n",
    "            'simple_average': simple_avg,\n",
    "            'weighted_average': weighted_avg,\n",
    "            'sentiment_adjusted': sentiment_adjusted,\n",
    "            'volatility_weighted': volatility_weighted,\n",
    "            'trend_following': trend_following,\n",
    "            'meta_ensemble': meta_ensemble\n",
    "        },\n",
    "        'percentage_changes': pct_changes,\n",
    "        'weights_used': weights_used,\n",
    "        'metadata': metadata,\n",
    "        'model_info': {\n",
    "            **ensemble_model.get('model_info', {}),\n",
    "            'created_date': datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "            'individual_predictions': {\n",
    "                'arimax': arimax_pred,\n",
    "                'xgboost': xgb_pred,\n",
    "                'random_forest': rf_pred,\n",
    "                'lstm': lstm_pred,\n",
    "                'llm': llm_pred\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d79b5348",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_latest_bullionvault_articles(URL=\"https://www.bullionvault.com/gold-news\"):\n",
    "    response = requests.get(URL)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    latest=soup.find(id='views-bootstrap-grid-1').find_all(class_='field-content')\n",
    "    list_data = []\n",
    "    for item in latest:\n",
    "        date=item.find(class_='views-field-created')\n",
    "        if not date:\n",
    "            continue\n",
    "        link=item.find(class_='views-field-title').find('a')['href']\n",
    "        page_response = requests.get(link)\n",
    "        page_soup = BeautifulSoup(page_response.content, 'html.parser')\n",
    "        content = page_soup.find('div', class_='field field-name-body field-type-text-with-summary field-label-hidden')\n",
    "        title = page_soup.find('h1').text.strip()\n",
    "        content_text = content.text.strip() if content else ''\n",
    "        data_point = {'Date': date.text.strip() if date else 'N/A', 'Content': title + ':' + content_text}\n",
    "        list_data.append(data_point)\n",
    "    list_df=pd.DataFrame(list_data)\n",
    "    list_df['Date']= pd.to_datetime(list_df['Date'],errors='coerce').dt.date\n",
    "    return list_df\n",
    "\n",
    "def yf_extract_info(item):\n",
    "    link=item.find('a',class_='subtle-link')['href']\n",
    "    title=item.find('a',class_='subtle-link')['title']\n",
    "\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--headless\")  # Run in headless mode\n",
    "    chrome_options.add_argument(\"--disable-gpu\")  # Disable GPU acceleration\n",
    "\n",
    "    page_driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=chrome_options)\n",
    "    page_driver.get(link)\n",
    "\n",
    "    page_soup = BeautifulSoup(page_driver.page_source, 'html.parser')\n",
    "    content = page_soup.find('div', class_='body')\n",
    "    content_text = content.text.strip() if content else ''\n",
    "    date= page_soup.find('div', class_= lambda c: c and c.startswith(\"byline\")).find('time')\n",
    "    data_point = {'Date': date.text.strip() if date else 'N/A', 'Content': title + ':' + content_text}\n",
    "    page_driver.quit()\n",
    "    return data_point\n",
    "\n",
    "def get_latest_yf_articles(URL=\"https://finance.yahoo.com/news/\"):\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--headless\")  # Run in headless mode\n",
    "    chrome_options.add_argument(\"--disable-gpu\")  # Disable GPU acceleration\n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=chrome_options)\n",
    "    driver.get(URL)\n",
    "    time.sleep(2)\n",
    "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    count = 0\n",
    "    while True:\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(2)  # Wait for new content to load\n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        if new_height == last_height:\n",
    "            break\n",
    "        last_height = new_height\n",
    "        count += 1\n",
    "        if count > 1:\n",
    "            break\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    articles = soup.find_all('li', class_='story-item')\n",
    "    list_data = []\n",
    "    #print(f\"Found {len(articles)} articles on Yahoo Finance.\")\n",
    "    for article in articles:\n",
    "        try:\n",
    "            list_data.append(yf_extract_info(article))\n",
    "        except Exception as e:\n",
    "            continue\n",
    "    driver.quit()\n",
    "    list_df=pd.DataFrame(list_data)\n",
    "    list_df['Date'].dropna(inplace=True)\n",
    "    #list_df['Date']= pd.to_datetime(list_df['Date'],errors='coerce').dt.date\n",
    "    return list_df\n",
    "\n",
    "def get_reuters_article_text(item, base_URL=\"https://www.reuters.com\"):\n",
    "    title = item.get_text(strip=True)\n",
    "    link = item.find('a', href=True)['href']\n",
    "    if not link.startswith('http'):\n",
    "        link = base_URL + link\n",
    "    chrome_options = Options()\n",
    "    driver = webdriver.Chrome(options=chrome_options)\n",
    "    driver.get(link)\n",
    "    time.sleep(2)\n",
    "    page_soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    driver.quit()\n",
    "    article = page_soup.find_all(\"div\", class_=lambda c: c and c.startswith(\"article-body__paragraph\"))\n",
    "    article_text = \"\"\n",
    "    for para in article:\n",
    "        paragraph_text = para.get_text(strip=True)\n",
    "        article_text = article_text + \".\" + paragraph_text\n",
    "    date = page_soup.find(\"span\", class_=lambda c: c and c.startswith(\"date-line__date\")).get_text(strip=True)\n",
    "    data_point = {'Date': date, 'Content': title + ':' + article_text}\n",
    "    return data_point\n",
    "\n",
    "def get_reuters_articles_list(URL):\n",
    "    chrome_options = Options()\n",
    "    driver = webdriver.Chrome(options=chrome_options)\n",
    "    driver.get(URL)\n",
    "    time.sleep(2)\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    driver.quit()\n",
    "    articles=soup.find_all(\"div\", class_=lambda c: c and c.startswith(\"story-card__area-headline\"))\n",
    "    list_data = []\n",
    "    for article in articles:\n",
    "        try:\n",
    "            list_data.append(get_reuters_article_text(article))\n",
    "        except Exception as e:\n",
    "            continue\n",
    "    list_df=pd.DataFrame(list_data)\n",
    "    list_df['Date']= pd.to_datetime(list_df['Date'],errors='coerce').dt.date\n",
    "    return list_df\n",
    "\n",
    "def get_reuters_articles():\n",
    "    base_URL=\"https://www.reuters.com\"\n",
    "    search_query=\"/site-search/?query=gold\"\n",
    "    df = pd.DataFrame(columns=['Date', 'Content'])\n",
    "    for section_val in ['all']:\n",
    "        for offset_nb in range(0, 40, 20):\n",
    "            offset =f\"&offset={offset_nb}\"\n",
    "            section=f\"&section={section_val}\"\n",
    "            URL = base_URL + search_query + offset + section\n",
    "            try:\n",
    "                df_latest=get_reuters_articles_list(URL)\n",
    "                df = pd.concat([df, df_latest], ignore_index=True)\n",
    "            except Exception as e:\n",
    "                print(f\"Error fetching articles from {URL}: {e}\")\n",
    "                continue\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5c235a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================================\n",
    "# Step 1: Check if today's model exists\n",
    "# =========================================================================\n",
    "today = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "yesterday = (datetime.now() - timedelta(days=1)).strftime(\"%Y-%m-%d\")\n",
    "today = yesterday\n",
    "next_day = (datetime.now() + timedelta(days=1)).strftime(\"%Y-%m-%d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca1620d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data \n",
      "\n",
      "                 Open       High        Low      Close  Volume   Returns  \\\n",
      "Date                                                                       \n",
      "2025-06-20  82.790001  82.790001  82.029999  82.230003  550293 -0.006524   \n",
      "\n",
      "              MA_5   MA_20    MA_50  Volatility  ...   BB_lower  BB_width  \\\n",
      "Date                                             ...                        \n",
      "2025-06-20  82.692  81.167  79.9768    0.007811  ...  78.625844  5.082312   \n",
      "\n",
      "            BB_position      MACD  MACD_Signal  MACD_Hist  Momentum_10  \\\n",
      "Date                                                                     \n",
      "2025-06-20     0.709157  0.943162     0.861658   0.081504     1.030006   \n",
      "\n",
      "              ROC_10  Sentiment  Sentiment_Label  \n",
      "Date                                              \n",
      "2025-06-20  0.012685      -0.75         negative  \n",
      "\n",
      "[1 rows x 22 columns]\n",
      "\n",
      "\n",
      "Number of rows in df: (6491, 6)\n",
      "Max articles per day: 22\n",
      "Encodings shape: (1648, 22, 512), Price changes shape: (1648, 1), Masks shape: (1648, 22)\n",
      "Dataset <modules.modules.VariableSetDataset object at 0x0000022A1E0B4160>\n"
     ]
    }
   ],
   "source": [
    "## TODO [Jaison]: Replace this with today's gold price Scraping. \n",
    "\n",
    "\n",
    "# Load Datasets for Time Series Models and News LLM Model\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "input_data = \"data/GOLDBEES_ETF_price_data_technical_indicators_sentiment.csv\"\n",
    "df = pd.read_csv(input_data, index_col=0, parse_dates=True)\n",
    "gold_today = df.iloc[[-1]]\n",
    "print(f\"Data \\n\\n{gold_today.head()}\\n\\n\")\n",
    "\n",
    "# ---------------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c01de52",
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO [Yaswanth] : Replace this with today's news articles scraping.\n",
    "\n",
    "def extract_news_data():\n",
    "    bullion_df = get_latest_bullionvault_articles()\n",
    "    yf_df=get_latest_yf_articles()\n",
    "    yf_df['Date']=pd.to_datetime(yf_df['Date'],errors='coerce').dt.date\n",
    "    reuters_df = get_reuters_articles()\n",
    "    three_days_ago = pd.to_datetime('today').date() - timedelta(days=3)\n",
    "\n",
    "    df_combined = pd.concat([bullion_df, yf_df, reuters_df], ignore_index=True)\n",
    "    df_combined = df_combined.sort_values(by='Date')\n",
    "    df_combined=df_combined[df_combined['Date'] >= three_days_ago]\n",
    "    # Placeholder for actual news data extraction logic\n",
    "    return df_combined\n",
    "\n",
    "## TODO [Tejashwini] : cleaning script for scraped news data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ce4cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO [Adithya] : Insert Topic extraction model here.\n",
    "## Extract the encodings.\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "## Load the Universal Sentence Encoder model\n",
    "model_url = \"https://tfhub.dev/google/universal-sentence-encoder/4\" #This is around 1 GB in size, it took a while for me to run this.\n",
    "embed = hub.load(model_url)\n",
    "\n",
    "# Generate embeddings\n",
    "embeddings = embed(sentences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4881c796",
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO [Mohan]: Integrate sentiment extraction model here\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5cb51d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## TODO [Tarun] : Replace the file read with values produced by previous members in the chain.\n",
    "\n",
    "news_llm_model_data = pd.read_pickle('data/combined_dataset_with_price_change.pkl')\n",
    "print(\"Number of rows in df:\",news_llm_model_data.shape)\n",
    "news_llm_model_data.head()\n",
    "\n",
    "#Group input data into sets for use in model.\n",
    "encodings, price_changes, masks = group_into_variable_sets(news_llm_model_data)\n",
    "print(f\"Encodings shape: {encodings.shape}, Price changes shape: {price_changes.shape}, Masks shape: {masks.shape}\")\n",
    "\n",
    "# Create the dataset\n",
    "dataset = VariableSetDataset(encodings, price_changes, masks)\n",
    "print(f\"Dataset {dataset}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "96f47182",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Get device for PyTorch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1b3460e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ARIMAX: Predicted gold price for 2025-06-23: 82.2040508858499\n",
      "Random Forest: Predicted gold price for 2025-06-23: 82.63219047144733\n",
      "XGBoost: Predicted gold price for 2025-06-23: 83.2262346595262\n",
      "LSTM: Predicted gold price for 2025-06-23: 82.97583439662782\n"
     ]
    }
   ],
   "source": [
    "# Run Time Series Models and get predictions\n",
    "# --------------------------------------------------------------------\n",
    "# Load pre-trained models\n",
    "arimax_model = sm.load_pickle(f'models/arimax_2025-06-20.pkl')\n",
    "random_forest_model = sm.load_pickle(f'models/random_forest_2025-06-20.pkl')\n",
    "xgboost_model = sm.load_pickle(f'models/xgboost_2025-06-20.pkl')\n",
    "\n",
    "lstw_model = LSTMModel(input_size=11).to(device)\n",
    "lstw_model.load_state_dict(torch.load('models/lstm_2025-06-20.pt', map_location=device))\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# Predict gold price using all four models with gold_price_prediction where possible\n",
    "\n",
    "# ARIMAX\n",
    "predicted_price_arimax = predict_next_day_gold_price_arimax(df, arimax_model)\n",
    "\n",
    "# Random Forest\n",
    "predicted_price_rf = predict_next_day_gold_price_rf(df, random_forest_model)\n",
    "\n",
    "# XGBoost\n",
    "predicted_price_xgb = predict_next_day_gold_price_xgboost(df, xgboost_model)\n",
    "\n",
    "# LSTM\n",
    "predicted_price_lstw = predict_next_day_gold_price_lstm(df, lstw_model)\n",
    "\n",
    "print(f\"ARIMAX: Predicted gold price for {next_day}: {predicted_price_arimax}\")\n",
    "\n",
    "print(f\"Random Forest: Predicted gold price for {next_day}: {predicted_price_rf}\")\n",
    "\n",
    "print(f\"XGBoost: Predicted gold price for {next_day}: {predicted_price_xgb}\")\n",
    "\n",
    "print(f\"LSTM: Predicted gold price for {next_day}: {predicted_price_lstw}\")\n",
    "# --------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ab2cd95b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from models/final_model.pth at epoch 100 with loss 0.4647\n"
     ]
    }
   ],
   "source": [
    "# Load the pre-trained News LLM model & get predictions\n",
    "# --------------------------------------------------------------------\n",
    "\n",
    "news_model = SetTransformer(\n",
    "    dim_input = 512, \n",
    "    num_outputs = 1, #One final prediction\n",
    "    dim_output = 1, #1D output for price change\n",
    "    num_inds=32, \n",
    "    dim_hidden=128, \n",
    "    num_heads=4, \n",
    "    ln=True #Layer normalization\n",
    "    ).to(device)\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# Load the pre-trained model weights\n",
    "checkpoint_path = 'models/final_model.pth'\n",
    "if os.path.exists(checkpoint_path):\n",
    "    start_epoch, start_loss = load_checkpoint(checkpoint_path, news_model, device)\n",
    "    print(f\"Model loaded from {checkpoint_path} at epoch {start_epoch} with loss {start_loss:.4f}\")\n",
    "else:\n",
    "    start_epoch, start_loss = 0, float('inf')\n",
    "    print(f\"No checkpoint found at {checkpoint_path}. Starting from scratch.\")\n",
    "# --------------------------------------------------------------------\n",
    "predicted_price_news_llm = 80 # Placeholder for the actual prediction logic\n",
    "#predicted_price_news_llm = news_model(news_llm_model_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "005714ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble Model Results for 2025-06-23:\n",
      "Predicted Price: 81.8896819508526\n",
      "Percentage Change: -0.41%\n"
     ]
    }
   ],
   "source": [
    "ensemble_model = sm.load_pickle(f'models/ensemble_model_2025-06-20.pkl')\n",
    "results = predict_next_day_gold_price_ensemble(\n",
    "    ensemble_model,\n",
    "    predicted_price_arimax,\n",
    "    predicted_price_xgb,\n",
    "    predicted_price_rf,\n",
    "    predicted_price_lstw,\n",
    "    predicted_price_news_llm,  # Placeholder for LLM prediction\n",
    ")\n",
    "\n",
    "print(f\"Ensemble Model Results for {next_day}:\")\n",
    "print(f\"Predicted Price: {results['predictions']['meta_ensemble']}\")\n",
    "print(f\"Percentage Change: {results['percentage_changes']['meta_ensemble']:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
